#  Text-JEPA: Joint Embedding Predictive Architecture for Text Representation Learning

> Text-JEPA is a self-supervised framework that adapts the Joint Embedding Predictive Architecture (JEPA) to text data. Instead of predicting masked tokens, Text-JEPA predicts embeddings of masked spans from surrounding context, enabling models to learn semantic and contextual representations without relying on token-level reconstruction. This approach improves semantic similarity, paraphrase robustness, and long-range reasoning in language tasks and can serve as a strong pretraining method for downstream NLP applications such as classification, retrieval, and natural language inference.


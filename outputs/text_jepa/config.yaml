data:
  batch_size: 32
  dataset_name: wikitext
  dataset_path: null
  max_seq_len: 512
  num_workers: 4
  vocab_size: 30522
logging:
  folder: ./outputs/text_jepa
  write_tag: text_jepa_experiment
mask:
  allow_overlap: false
  enc_mask_scale:
  - 0.65
  - 0.85
  max_tokens: 512
  min_keep: 32
  num_enc_masks: 1
  num_pred_masks: 2
  pred_mask_scale:
  - 0.1
  - 0.25
meta:
  load_checkpoint: false
  model_name: text_transformer_small
  pred_depth: 6
  pred_emb_dim: 192
  read_checkpoint: null
  use_bfloat16: true
optimization:
  ema:
  - 0.996
  - 1.0
  epochs: 100
  final_lr: 1.0e-06
  final_weight_decay: 0.05
  ipe_scale: 1.0
  lr: 5.0e-05
  start_lr: 1.0e-06
  warmup: 10
  weight_decay: 0.05
